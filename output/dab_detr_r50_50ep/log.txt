[01/18 14:41:29] detectron2 INFO: Rank of current process: 0. World size: 1
[01/18 14:41:30] detectron2 INFO: Environment info:
----------------------  ---------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) [GCC 9.4.0]
numpy                   1.21.5
detectron2              0.6 @/home/fangyi/research_delta/sqr-detrex/detectron2/detectron2
Compiler                GCC 5.4
CUDA compiler           CUDA 11.1
detectron2 arch flags   7.5
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.10.0 @/home/fangyi/anaconda3/envs/research-charlie-detrex/lib/python3.7/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   GeForce RTX 2080 Ti (arch=7.5)
Driver version          455.23.05
CUDA_HOME               /usr/local/cuda-11.1
Pillow                  9.2.0
torchvision             0.11.1 @/home/fangyi/anaconda3/envs/research-charlie-detrex/lib/python3.7/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220512
iopath                  0.1.9
cv2                     4.6.0
----------------------  ---------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/18 14:41:30] detectron2 INFO: Command line arguments: Namespace(config_file='projects/dab_detr/configs/dab_detr_r50_50ep.py', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[01/18 14:41:30] detectron2 INFO: Contents of args.config_file=projects/dab_detr/configs/dab_detr_r50_50ep.py:
from detrex.config import get_config
from .models.dab_detr_r50 import model

dataloader = get_config("common/data/coco_detr.py").dataloader
optimizer = get_config("common/optim.py").AdamW
lr_multiplier = get_config("common/coco_schedule.py").lr_multiplier_50ep
train = get_config("common/train.py").train

# initialize checkpoint to be loaded
train.init_checkpoint = "detectron2://ImageNetPretrained/torchvision/R-50.pkl"
train.output_dir = "./output/dab_detr_r50_50ep"

# max training iterations
train.max_iter = 375000

# run evaluation every 5000 iters
train.eval_period = 5000

# log training infomation every 20 iters
train.log_period = 20

# save checkpoint every 5000 iters
train.checkpointer.period = 5000

# gradient clipping for training
train.clip_grad.enabled = True
train.clip_grad.params.max_norm = 0.1
train.clip_grad.params.norm_type = 2

# set training devices
train.device = "cuda"
model.device = train.device

# modify optimizer config
optimizer.lr = 1e-4
optimizer.betas = (0.9, 0.999)
optimizer.weight_decay = 1e-4
optimizer.params.lr_factor_func = lambda module_name: 0.1 if "backbone" in module_name else 1

# modify dataloader config
dataloader.train.num_workers = 16

# please notice that this is total batch size.
# surpose you're using 4 gpus for training and the batch size for
# each gpu is 16/4 = 4
dataloader.train.total_batch_size = 16

# dump the testing results into output_dir for visualization
dataloader.evaluator.output_dir = train.output_dir

[01/18 14:41:30] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/dab_detr_r50_50ep/config.yaml is human-readable but cannot be loaded.
[01/18 14:41:30] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/dab_detr_r50_50ep/config.yaml.pkl.
[01/18 14:41:30] detectron2 INFO: Full config saved to ./output/dab_detr_r50_50ep/config.yaml
[01/18 14:41:30] d2.utils.env INFO: Using a generated random seed 30337640
[01/18 14:41:32] detectron2 INFO: Model:
DABDETR(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (position_embedding): PositionEmbeddingSine()
  (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
  (transformer): DabDetrTransformer(
    (encoder): DabDetrTransformerEncoder(
      (layers): ModuleList(
        (0): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): PReLU(num_parameters=1)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): PReLU(num_parameters=1)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): PReLU(num_parameters=1)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): PReLU(num_parameters=1)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): PReLU(num_parameters=1)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): PReLU(num_parameters=1)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): PReLU(num_parameters=1)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): PReLU(num_parameters=1)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): PReLU(num_parameters=1)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): PReLU(num_parameters=1)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): PReLU(num_parameters=1)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): PReLU(num_parameters=1)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (query_scale): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
    (decoder): DabDetrTransformerDecoder(
      (layers): ModuleList(
        (0): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): ConditionalSelfAttention(
              (query_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (query_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): ConditionalCrossAttention(
              (query_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (query_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (query_pos_sine_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): PReLU(num_parameters=1)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): PReLU(num_parameters=1)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): ConditionalSelfAttention(
              (query_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (query_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): ConditionalCrossAttention(
              (query_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (query_pos_proj): None
              (query_pos_sine_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): PReLU(num_parameters=1)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): PReLU(num_parameters=1)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): ConditionalSelfAttention(
              (query_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (query_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): ConditionalCrossAttention(
              (query_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (query_pos_proj): None
              (query_pos_sine_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): PReLU(num_parameters=1)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): PReLU(num_parameters=1)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): ConditionalSelfAttention(
              (query_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (query_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): ConditionalCrossAttention(
              (query_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (query_pos_proj): None
              (query_pos_sine_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): PReLU(num_parameters=1)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): PReLU(num_parameters=1)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): ConditionalSelfAttention(
              (query_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (query_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): ConditionalCrossAttention(
              (query_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (query_pos_proj): None
              (query_pos_sine_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): PReLU(num_parameters=1)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): PReLU(num_parameters=1)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): BaseTransformerLayer(
          (attentions): ModuleList(
            (0): ConditionalSelfAttention(
              (query_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (query_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): ConditionalCrossAttention(
              (query_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (query_pos_proj): None
              (query_pos_sine_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_content_proj): Linear(in_features=256, out_features=256, bias=True)
              (key_pos_proj): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): PReLU(num_parameters=1)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): PReLU(num_parameters=1)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (query_scale): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ref_point_head): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=512, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ref_anchor_head): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (post_norm_layer): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bbox_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (anchor_box_embed): Embedding(300, 4)
  (class_embed): Linear(in_features=256, out_features=80, bias=True)
  (bbox_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=4, bias=True)
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_bbox: 5.0
          cost_giou: 2.0
          cost_class_type: focal_loss_cost
          focal cost alpha: 0.25
          focal cost gamma: 2.0
      losses: ['class', 'boxes']
      loss_class_type: focal_loss
      weight_dict: {'loss_class': 1, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_class_0': 1, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_class_1': 1, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_class_2': 1, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_class_3': 1, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_class_4': 1, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0}
      num_classes: 80
      eos_coef: 0.1
      focal loss alpha: 0.25
      focal loss gamma: 2.0
)
[01/18 14:41:43] d2.data.datasets.coco INFO: Loading /home/fangyi/data/coco/annotations/instances_train2017.json takes 11.09 seconds.
[01/18 14:41:44] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from /home/fangyi/data/coco/annotations/instances_train2017.json
[01/18 14:41:49] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[01/18 14:41:54] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[01/18 14:41:54] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[01/18 14:41:56] d2.data.common INFO: Serialized dataset takes 452.22 MiB
[01/18 14:41:59] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[01/18 14:41:59] fvcore.common.checkpoint INFO: Reading a file from 'torchvision'
[01/18 14:41:59] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint                                                               | Shapes                                          |
|:------------------|:----------------------------------------------------------------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.*      | stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (64,) (64,) (64,) (64,) (64,3,7,7)              |
[01/18 14:41:59] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34manchor_box_embed.weight[0m
[34mbbox_embed.layers.0.{bias, weight}[0m
[34mbbox_embed.layers.1.{bias, weight}[0m
[34mbbox_embed.layers.2.{bias, weight}[0m
[34mclass_embed.{bias, weight}[0m
[34minput_proj.{bias, weight}[0m
[34mtransformer.decoder.bbox_embed.layers.0.{bias, weight}[0m
[34mtransformer.decoder.bbox_embed.layers.1.{bias, weight}[0m
[34mtransformer.decoder.bbox_embed.layers.2.{bias, weight}[0m
[34mtransformer.decoder.layers.0.attentions.0.key_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.0.attentions.0.key_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.0.attentions.0.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.0.attentions.0.query_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.0.attentions.0.query_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.0.attentions.0.value_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.0.attentions.1.key_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.0.attentions.1.key_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.0.attentions.1.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.0.attentions.1.query_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.0.attentions.1.query_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.0.attentions.1.query_pos_sine_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.0.attentions.1.value_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.0.ffns.0.activation.weight[0m
[34mtransformer.decoder.layers.0.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.decoder.layers.0.ffns.0.layers.0.1.weight[0m
[34mtransformer.decoder.layers.0.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.decoder.layers.0.norms.0.{bias, weight}[0m
[34mtransformer.decoder.layers.0.norms.1.{bias, weight}[0m
[34mtransformer.decoder.layers.0.norms.2.{bias, weight}[0m
[34mtransformer.decoder.layers.1.attentions.0.key_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.1.attentions.0.key_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.1.attentions.0.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.1.attentions.0.query_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.1.attentions.0.query_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.1.attentions.0.value_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.1.attentions.1.key_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.1.attentions.1.key_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.1.attentions.1.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.1.attentions.1.query_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.1.attentions.1.query_pos_sine_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.1.attentions.1.value_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.1.ffns.0.activation.weight[0m
[34mtransformer.decoder.layers.1.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.decoder.layers.1.ffns.0.layers.0.1.weight[0m
[34mtransformer.decoder.layers.1.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.decoder.layers.1.norms.0.{bias, weight}[0m
[34mtransformer.decoder.layers.1.norms.1.{bias, weight}[0m
[34mtransformer.decoder.layers.1.norms.2.{bias, weight}[0m
[34mtransformer.decoder.layers.2.attentions.0.key_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.2.attentions.0.key_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.2.attentions.0.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.2.attentions.0.query_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.2.attentions.0.query_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.2.attentions.0.value_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.2.attentions.1.key_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.2.attentions.1.key_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.2.attentions.1.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.2.attentions.1.query_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.2.attentions.1.query_pos_sine_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.2.attentions.1.value_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.2.ffns.0.activation.weight[0m
[34mtransformer.decoder.layers.2.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.decoder.layers.2.ffns.0.layers.0.1.weight[0m
[34mtransformer.decoder.layers.2.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.decoder.layers.2.norms.0.{bias, weight}[0m
[34mtransformer.decoder.layers.2.norms.1.{bias, weight}[0m
[34mtransformer.decoder.layers.2.norms.2.{bias, weight}[0m
[34mtransformer.decoder.layers.3.attentions.0.key_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.3.attentions.0.key_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.3.attentions.0.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.3.attentions.0.query_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.3.attentions.0.query_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.3.attentions.0.value_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.3.attentions.1.key_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.3.attentions.1.key_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.3.attentions.1.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.3.attentions.1.query_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.3.attentions.1.query_pos_sine_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.3.attentions.1.value_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.3.ffns.0.activation.weight[0m
[34mtransformer.decoder.layers.3.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.decoder.layers.3.ffns.0.layers.0.1.weight[0m
[34mtransformer.decoder.layers.3.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.decoder.layers.3.norms.0.{bias, weight}[0m
[34mtransformer.decoder.layers.3.norms.1.{bias, weight}[0m
[34mtransformer.decoder.layers.3.norms.2.{bias, weight}[0m
[34mtransformer.decoder.layers.4.attentions.0.key_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.4.attentions.0.key_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.4.attentions.0.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.4.attentions.0.query_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.4.attentions.0.query_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.4.attentions.0.value_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.4.attentions.1.key_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.4.attentions.1.key_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.4.attentions.1.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.4.attentions.1.query_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.4.attentions.1.query_pos_sine_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.4.attentions.1.value_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.4.ffns.0.activation.weight[0m
[34mtransformer.decoder.layers.4.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.decoder.layers.4.ffns.0.layers.0.1.weight[0m
[34mtransformer.decoder.layers.4.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.decoder.layers.4.norms.0.{bias, weight}[0m
[34mtransformer.decoder.layers.4.norms.1.{bias, weight}[0m
[34mtransformer.decoder.layers.4.norms.2.{bias, weight}[0m
[34mtransformer.decoder.layers.5.attentions.0.key_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.5.attentions.0.key_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.5.attentions.0.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.5.attentions.0.query_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.5.attentions.0.query_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.5.attentions.0.value_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.5.attentions.1.key_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.5.attentions.1.key_pos_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.5.attentions.1.out_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.5.attentions.1.query_content_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.5.attentions.1.query_pos_sine_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.5.attentions.1.value_proj.{bias, weight}[0m
[34mtransformer.decoder.layers.5.ffns.0.activation.weight[0m
[34mtransformer.decoder.layers.5.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.decoder.layers.5.ffns.0.layers.0.1.weight[0m
[34mtransformer.decoder.layers.5.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.decoder.layers.5.norms.0.{bias, weight}[0m
[34mtransformer.decoder.layers.5.norms.1.{bias, weight}[0m
[34mtransformer.decoder.layers.5.norms.2.{bias, weight}[0m
[34mtransformer.decoder.post_norm_layer.{bias, weight}[0m
[34mtransformer.decoder.query_scale.layers.0.{bias, weight}[0m
[34mtransformer.decoder.query_scale.layers.1.{bias, weight}[0m
[34mtransformer.decoder.ref_anchor_head.layers.0.{bias, weight}[0m
[34mtransformer.decoder.ref_anchor_head.layers.1.{bias, weight}[0m
[34mtransformer.decoder.ref_point_head.layers.0.{bias, weight}[0m
[34mtransformer.decoder.ref_point_head.layers.1.{bias, weight}[0m
[34mtransformer.encoder.layers.0.attentions.0.attn.out_proj.{bias, weight}[0m
[34mtransformer.encoder.layers.0.attentions.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.encoder.layers.0.ffns.0.activation.weight[0m
[34mtransformer.encoder.layers.0.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.encoder.layers.0.ffns.0.layers.0.1.weight[0m
[34mtransformer.encoder.layers.0.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.encoder.layers.0.norms.0.{bias, weight}[0m
[34mtransformer.encoder.layers.0.norms.1.{bias, weight}[0m
[34mtransformer.encoder.layers.1.attentions.0.attn.out_proj.{bias, weight}[0m
[34mtransformer.encoder.layers.1.attentions.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.encoder.layers.1.ffns.0.activation.weight[0m
[34mtransformer.encoder.layers.1.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.encoder.layers.1.ffns.0.layers.0.1.weight[0m
[34mtransformer.encoder.layers.1.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.encoder.layers.1.norms.0.{bias, weight}[0m
[34mtransformer.encoder.layers.1.norms.1.{bias, weight}[0m
[34mtransformer.encoder.layers.2.attentions.0.attn.out_proj.{bias, weight}[0m
[34mtransformer.encoder.layers.2.attentions.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.encoder.layers.2.ffns.0.activation.weight[0m
[34mtransformer.encoder.layers.2.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.encoder.layers.2.ffns.0.layers.0.1.weight[0m
[34mtransformer.encoder.layers.2.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.encoder.layers.2.norms.0.{bias, weight}[0m
[34mtransformer.encoder.layers.2.norms.1.{bias, weight}[0m
[34mtransformer.encoder.layers.3.attentions.0.attn.out_proj.{bias, weight}[0m
[34mtransformer.encoder.layers.3.attentions.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.encoder.layers.3.ffns.0.activation.weight[0m
[34mtransformer.encoder.layers.3.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.encoder.layers.3.ffns.0.layers.0.1.weight[0m
[34mtransformer.encoder.layers.3.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.encoder.layers.3.norms.0.{bias, weight}[0m
[34mtransformer.encoder.layers.3.norms.1.{bias, weight}[0m
[34mtransformer.encoder.layers.4.attentions.0.attn.out_proj.{bias, weight}[0m
[34mtransformer.encoder.layers.4.attentions.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.encoder.layers.4.ffns.0.activation.weight[0m
[34mtransformer.encoder.layers.4.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.encoder.layers.4.ffns.0.layers.0.1.weight[0m
[34mtransformer.encoder.layers.4.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.encoder.layers.4.norms.0.{bias, weight}[0m
[34mtransformer.encoder.layers.4.norms.1.{bias, weight}[0m
[34mtransformer.encoder.layers.5.attentions.0.attn.out_proj.{bias, weight}[0m
[34mtransformer.encoder.layers.5.attentions.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mtransformer.encoder.layers.5.ffns.0.activation.weight[0m
[34mtransformer.encoder.layers.5.ffns.0.layers.0.0.{bias, weight}[0m
[34mtransformer.encoder.layers.5.ffns.0.layers.0.1.weight[0m
[34mtransformer.encoder.layers.5.ffns.0.layers.1.{bias, weight}[0m
[34mtransformer.encoder.layers.5.norms.0.{bias, weight}[0m
[34mtransformer.encoder.layers.5.norms.1.{bias, weight}[0m
[34mtransformer.encoder.query_scale.layers.0.{bias, weight}[0m
[34mtransformer.encoder.query_scale.layers.1.{bias, weight}[0m
[01/18 14:41:59] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.fc.{bias, weight}[0m
[01/18 14:41:59] d2.engine.train_loop INFO: Starting training from iteration 0
[01/18 14:42:01] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/fangyi/research_delta/sqr-detrex/detectron2/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "tools/train_net.py", line 101, in run_step
    loss_dict = self.model(data)
  File "/home/fangyi/anaconda3/envs/research-charlie-detrex/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/fangyi/research_delta/sqr-detrex/projects/dab_detr/modeling/dab_detr.py", line 170, in forward
    features = self.backbone(images.tensor)[self.in_features[-1]]
  File "/home/fangyi/anaconda3/envs/research-charlie-detrex/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/fangyi/research_delta/sqr-detrex/detrex/modeling/backbone/resnet.py", line 468, in forward
    x = stage(x)
  File "/home/fangyi/anaconda3/envs/research-charlie-detrex/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/fangyi/anaconda3/envs/research-charlie-detrex/lib/python3.7/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/fangyi/anaconda3/envs/research-charlie-detrex/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/fangyi/research_delta/sqr-detrex/detrex/modeling/backbone/resnet.py", line 218, in forward
    out = self.conv3(out)
  File "/home/fangyi/anaconda3/envs/research-charlie-detrex/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/fangyi/research_delta/sqr-detrex/detectron2/detectron2/layers/wrappers.py", line 117, in forward
    x = self.norm(x)
  File "/home/fangyi/anaconda3/envs/research-charlie-detrex/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/fangyi/research_delta/sqr-detrex/detectron2/detectron2/layers/batch_norm.py", line 53, in forward
    return x * scale.to(out_dtype) + bias.to(out_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 414.00 MiB (GPU 0; 10.76 GiB total capacity; 6.11 GiB already allocated; 279.69 MiB free; 6.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[01/18 14:42:01] d2.engine.hooks INFO: Total training time: 0:00:01 (0:00:00 on hooks)
[01/18 14:42:01] d2.utils.events INFO:  iter: 0    lr: N/A  max_mem: 6467M
